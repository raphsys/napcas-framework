Directory structure:
└── raphsys-napcas-framework/
    ├── CMakeLists.txt
    ├── cpp/
    │   ├── include/
    │   │   ├── activation.h
    │   │   ├── autograd.h
    │   │   ├── conv2d.h
    │   │   ├── data_loader.h
    │   │   ├── linear.h
    │   │   ├── loss.h
    │   │   ├── module.h
    │   │   ├── napca_sim.h
    │   │   ├── napcas.h
    │   │   ├── nncell.h
    │   │   ├── optimizer.h
    │   │   └── tensor.h
    │   └── src/
    │       ├── activation.cpp
    │       ├── autograd.cpp
    │       ├── conv2d.cpp
    │       ├── data_loader.cpp
    │       ├── linear.cpp
    │       ├── loss.cpp
    │       ├── napca_sim.cpp
    │       ├── napcas.cpp
    │       ├── nncell.cpp
    │       ├── optimizer.cpp
    │       ├── python_bindings.cpp
    │       └── tensor.cpp
    ├── docs/
    ├── python/
    │   ├── __init__.py
    │   ├── pyproject.toml
    │   ├── setup.py
    │   ├── test_dataloader_autograd.py
    │   ├── test_napcas.py
    │   └── test_napcas_components.py
    └── tests/
        ├── CMakeLists.txt
        ├── test_all_modules.cpp
        ├── test_all_modules.py
        ├── test_napcas.cpp
        └── test_napcas.py

================================================
FILE: CMakeLists.txt
================================================
cmake_minimum_required(VERSION 3.14)
project(napcas)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# Compilation sécurisée
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -fPIC -fvisibility=hidden -fvisibility-inlines-hidden")

# Activer PYBIND11_FINDPYTHON pour éviter l'avertissement CMP0148
set(PYBIND11_FINDPYTHON ON)

# Trouver pybind11 et Python
find_package(pybind11 REQUIRED)
find_package(Python3 REQUIRED COMPONENTS Development)

# Trouver Eigen
find_package(Eigen3 REQUIRED)
include_directories(${EIGEN3_INCLUDE_DIR})

# Définir les sources de la bibliothèque C++
set(NAPCAS_SRC
    cpp/src/activation.cpp
    cpp/src/autograd.cpp
    cpp/src/conv2d.cpp
    cpp/src/data_loader.cpp
    cpp/src/linear.cpp
    cpp/src/loss.cpp
    cpp/src/napcas.cpp
    cpp/src/napca_sim.cpp
    cpp/src/nncell.cpp
    cpp/src/optimizer.cpp
    cpp/src/tensor.cpp
)

# Vérifier que les fichiers source existent
foreach(src_file ${NAPCAS_SRC})
    if(NOT EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/${src_file}")
        message(WARNING "Source file ${src_file} not found")
    endif()
endforeach()

# Vérifier que python_bindings.cpp existe
if(NOT EXISTS "${CMAKE_CURRENT_SOURCE_DIR}/cpp/src/python_bindings.cpp")
    message(FATAL_ERROR "Source file cpp/src/python_bindings.cpp not found")
endif()

# Ajouter le chemin du répertoire contenant la bibliothèque
link_directories(${CMAKE_CURRENT_SOURCE_DIR}/../build)

# Créer la bibliothèque statique
add_library(libnapcas STATIC ${NAPCAS_SRC})
target_include_directories(libnapcas PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/cpp/include ${EIGEN3_INCLUDE_DIR})
target_link_libraries(libnapcas PUBLIC Eigen3::Eigen)


# Créer le module Python
pybind11_add_module(napcas MODULE cpp/src/python_bindings.cpp)
target_link_libraries(napcas PRIVATE libnapcas Python3::Python Eigen3::Eigen)

# Propriétés de visibilité
set_target_properties(napcas PROPERTIES
    CXX_VISIBILITY_PRESET "hidden"
    VISIBILITY_INLINES_HIDDEN ON
)

# Ajouter les tests si la variable BUILD_TESTS est définie
if(BUILD_TESTS)
    add_subdirectory(tests)
endif()





================================================
FILE: cpp/include/activation.h
================================================
#ifndef ACTIVATION_H
#define ACTIVATION_H

#include "module.h"
#include "tensor.h"

class ReLU : public Module {
public:
    ReLU();
    void forward(Tensor& input, Tensor& output) override;
    void backward(Tensor& grad_output, Tensor& grad_input) override;
    void update(float lr) override;
    Tensor& get_weights() override;
    Tensor& get_grad_weights() override;
    void set_weights(const Tensor& weights) override;
};

class Sigmoid : public Module {
public:
    Sigmoid();
    void forward(Tensor& input, Tensor& output) override;
    void backward(Tensor& grad_output, Tensor& grad_input) override;
    void update(float lr) override;
    Tensor& get_weights() override;
    Tensor& get_grad_weights() override;
    void set_weights(const Tensor& weights) override;
};

class Tanh : public Module {
public:
    Tanh();
    void forward(Tensor& input, Tensor& output) override;
    void backward(Tensor& grad_output, Tensor& grad_input) override;
    void update(float lr) override;
    Tensor& get_weights() override;
    Tensor& get_grad_weights() override;
    void set_weights(const Tensor& weights) override;
};

#endif // ACTIVATION_H



================================================
FILE: cpp/include/autograd.h
================================================
#ifndef AUTOGRAD_H
#define AUTOGRAD_H

#include "tensor.h"

class Autograd {
public:
    static void zero_grad(std::vector<Tensor>& tensors);
};

#endif // AUTOGRAD_H



================================================
FILE: cpp/include/conv2d.h
================================================
#ifndef CONV2D_H
#define CONV2D_H

#include "module.h"
#include "tensor.h"

class Conv2d : public Module {
public:
    Conv2d(int in_channels, int out_channels, int kernel_size);
    void forward(Tensor& input, Tensor& output) override;
    void backward(Tensor& grad_output, Tensor& grad_input) override;
    void update(float lr) override;
    Tensor& get_weights() override;
    Tensor& get_grad_weights() override;
    void set_weights(const Tensor& weights) override;

private:
    int kernel_size_;
    Tensor weights_;
    Tensor bias_;
    Tensor grad_weights_;
    Tensor grad_bias_;
    float learning_rate_;
};

#endif // CONV2D_H




================================================
FILE: cpp/include/data_loader.h
================================================
#ifndef DATA_LOADER_H
#define DATA_LOADER_H

#include <vector>
#include <string>
#include "tensor.h"

class DataLoader {
public:
    DataLoader(const std::string& dataset_path, int batch_size);
    std::pair<Tensor, Tensor> next();

private:
    std::vector<std::vector<float>> data_; // Stocke les données brutes
    std::vector<Tensor> inputs_;           // Stocke les tenseurs d'entrée
    std::vector<Tensor> targets_;          // Stocke les tenseurs cible
    int batch_size_;
    int current_index_;
};

#endif // DATA_LOADER_H



================================================
FILE: cpp/include/linear.h
================================================
#ifndef LINEAR_H
#define LINEAR_H

#include "module.h"
#include "tensor.h"

class Linear : public Module {
public:
    Linear(int in_features, int out_features);
    void forward(Tensor& input, Tensor& output) override;
    void backward(Tensor& grad_output, Tensor& grad_input) override;
    void update(float lr) override;
    Tensor& get_weights() override;
    Tensor& get_grad_weights() override;
    void set_weights(const Tensor& weights) override;

private:
    Tensor weights_;
    Tensor bias_;
    Tensor grad_weights_;
    Tensor grad_bias_;
    float learning_rate_;
};

#endif // LINEAR_H



================================================
FILE: cpp/include/loss.h
================================================
#ifndef LOSS_H
#define LOSS_H

#include "tensor.h"

class MSELoss {
public:
    MSELoss() = default; // Explicit declaration
    float forward(Tensor& y_pred, Tensor& y_true);
    Tensor backward(Tensor& y_pred, Tensor& y_true);
};

class CrossEntropyLoss {
public:
    CrossEntropyLoss() = default; // Explicit declaration
    float forward(Tensor& y_pred, Tensor& y_true);
    Tensor backward(Tensor& y_pred, Tensor& y_true);
};

#endif // LOSS_H



================================================
FILE: cpp/include/module.h
================================================
#ifndef MODULE_H
#define MODULE_H

#include "tensor.h"
#include <memory>
#include <vector>

class Module {
public:
    virtual void forward(Tensor& input, Tensor& output) = 0;
    virtual void backward(Tensor& grad_output, Tensor& grad_input) = 0;
    virtual void update(float lr) = 0;
    virtual Tensor& get_weights() = 0;
    virtual Tensor& get_grad_weights() = 0;
    virtual void set_weights(const Tensor& weights) = 0; // Ajout de cette méthode
    virtual ~Module() = default;

protected:
    Tensor input_;
};

#endif // MODULE_H



================================================
FILE: cpp/include/napca_sim.h
================================================
#ifndef NAPCA_SIM_H
#define NAPCA_SIM_H

#include "module.h"
#include "tensor.h"
#include <vector>
#include <unordered_map>

class NAPCA_Sim : public Module {
public:
    NAPCA_Sim(int in_features, int out_features, float alpha=0.6f, float threshold=0.5f);
    void forward(Tensor& input, Tensor& output) override;
    void backward(Tensor& grad_output, Tensor& grad_input) override;
    void update(float lr) override;
    Tensor& get_weights() override;
    Tensor& get_grad_weights() override;
    void set_weights(const Tensor& weights) override;
    
    // Nouvelles fonctionnalités spécifiques à NAP-CA Sim
    float compute_path_similarity(const std::vector<int>& path1, const std::vector<int>& path2);
    void update_weights_conditionally(const std::vector<std::pair<int,int>>& similar_pairs, float eta);
    void prune_connections(float threshold=0.01f);

private:
    Tensor weights_;
    Tensor bias_;
    Tensor grad_weights_;
    Tensor grad_bias_;
    float learning_rate_;
    float alpha_;
    float threshold_;
    
    std::vector<std::vector<int>> memory_paths_;
    std::unordered_map<int, bool> active_connections_;
};

#endif // NAPCA_SIM_H



================================================
FILE: cpp/include/napcas.h
================================================
#ifndef NAPCAS_H
#define NAPCAS_H

#include "module.h"
#include "tensor.h"

class NAPCAS : public Module {
public:
    NAPCAS(int in_features, int out_features);
    void forward(Tensor& input, Tensor& output) override;
    void backward(Tensor& grad_output, Tensor& grad_input) override;
    void update(float lr) override;
    Tensor& get_weights() override;
    Tensor& get_grad_weights() override;
    void set_weights(const Tensor& weights) override;

private:
    Tensor weights_;
    Tensor bias_;
    Tensor grad_weights_;
    Tensor grad_bias_;
    float learning_rate_;
};

#endif // NAPCAS_H



================================================
FILE: cpp/include/nncell.h
================================================
#ifndef NNCELL_H
#define NNCELL_H

#include "module.h"
#include "tensor.h"

class NNCell : public Module {
public:
    NNCell(int in_features, int out_features);
    void forward(Tensor& input, Tensor& output) override;
    void backward(Tensor& grad_output, Tensor& grad_input) override;
    void update(float lr) override;
    Tensor& get_weights() override;
    Tensor& get_grad_weights() override;
    void set_weights(const Tensor& weights) override;

private:
    Tensor weights_;
    Tensor bias_;
    Tensor grad_weights_;
    Tensor grad_bias_;
    float learning_rate_;
};

#endif // NNCELL_H



================================================
FILE: cpp/include/optimizer.h
================================================
#ifndef OPTIMIZER_H
#define OPTIMIZER_H

#include <vector>
#include <memory>
#include "module.h"
#include "tensor.h"

class SGD {
public:
    SGD(float lr = 0.01f);
    SGD(const std::vector<std::shared_ptr<Module>>& modules, float lr = 0.01f);
    void step();

private:
    float learning_rate_;
    std::vector<std::shared_ptr<Module>> modules_;
};

class Adam {
public:
    Adam(float lr = 0.001f, float beta1 = 0.9f, float beta2 = 0.999f, float epsilon = 1e-8f);
    Adam(const std::vector<std::shared_ptr<Module>>& modules, float lr = 0.001f, float beta1 = 0.9f, float beta2 = 0.999f, float epsilon = 1e-8f);
    void step();

private:
    float learning_rate_;
    float beta1_, beta2_, epsilon_;
    int t_;
    std::vector<std::shared_ptr<Module>> modules_;
    std::vector<Tensor> m_; // Premier moment (moyenne)
    std::vector<Tensor> v_; // Second moment (variance)
};

#endif // OPTIMIZER_H



================================================
FILE: cpp/include/tensor.h
================================================
#ifndef TENSOR_H
#define TENSOR_H

#include <vector>
#include <stdexcept>

class Tensor {
public:
    Tensor() = default;
    Tensor(const std::vector<int>& shape, const std::vector<float>& data = {});
    int size() const { return data_.size(); }
    const std::vector<int>& shape() const { return shape_; }
    std::vector<float>& data() { return data_; }
    const std::vector<float>& data() const { return data_; }
    void fill(float value);
    void zero_grad();
    float& operator[](int i) { return data_[i]; }
    const float& operator[](int i) const { return data_[i]; }
    int ndim() const { return shape_.size(); }

private:
    std::vector<int> shape_;
    std::vector<float> data_;
};

#endif // TENSOR_H



================================================
FILE: cpp/src/activation.cpp
================================================
#include "activation.h"
#include <cmath>
#include <stdexcept>

ReLU::ReLU() {}

void ReLU::forward(Tensor& input, Tensor& output) {
    if (input.shape() != output.shape()) {
        throw std::invalid_argument("Input and output shapes must match");
    }
    for (int i = 0; i < input.size(); ++i) {
        output[i] = std::max(0.0f, input[i]);
    }
}

void ReLU::backward(Tensor& grad_output, Tensor& grad_input) {
    if (grad_output.shape() != grad_input.shape()) {
        throw std::invalid_argument("Gradient shapes must match");
    }
    for (int i = 0; i < grad_output.size(); ++i) {
        grad_input[i] = (grad_output[i] > 0) ? grad_output[i] : 0.0f;
    }
}

void ReLU::update(float) {}

Tensor& ReLU::get_weights() {
    throw std::runtime_error("ReLU has no weights");
}

Tensor& ReLU::get_grad_weights() {
    throw std::runtime_error("ReLU has no gradient weights");
}

void ReLU::set_weights(const Tensor& weights) {
    throw std::runtime_error("ReLU has no weights to set");
}

Sigmoid::Sigmoid() {}

void Sigmoid::forward(Tensor& input, Tensor& output) {
    if (input.shape() != output.shape()) {
        throw std::invalid_argument("Input and output shapes must match");
    }
    for (int i = 0; i < input.size(); ++i) {
        output[i] = 1.0f / (1.0f + std::exp(-input[i]));
    }
}

void Sigmoid::backward(Tensor& grad_output, Tensor& grad_input) {
    if (grad_output.shape() != grad_input.shape()) {
        throw std::invalid_argument("Gradient shapes must match");
    }
    for (int i = 0; i < grad_output.size(); ++i) {
        float sigmoid = 1.0f / (1.0f + std::exp(-grad_output[i]));
        grad_input[i] = grad_output[i] * sigmoid * (1.0f - sigmoid);
    }
}

void Sigmoid::update(float) {}

Tensor& Sigmoid::get_weights() {
    throw std::runtime_error("Sigmoid has no weights");
}

Tensor& Sigmoid::get_grad_weights() {
    throw std::runtime_error("Sigmoid has no gradient weights");
}

void Sigmoid::set_weights(const Tensor& weights) {
    throw std::runtime_error("Sigmoid has no weights to set");
}

Tanh::Tanh() {}

void Tanh::forward(Tensor& input, Tensor& output) {
    if (input.shape() != output.shape()) {
        throw std::invalid_argument("Input and output shapes must match");
    }
    for (int i = 0; i < input.size(); ++i) {
        output[i] = std::tanh(input[i]);
    }
}

void Tanh::backward(Tensor& grad_output, Tensor& grad_input) {
    if (grad_output.shape() != grad_input.shape()) {
        throw std::invalid_argument("Gradient shapes must match");
    }
    for (int i = 0; i < grad_output.size(); ++i) {
        float tanh_val = std::tanh(grad_output[i]);
        grad_input[i] = grad_output[i] * (1.0f - tanh_val * tanh_val);
    }
}

void Tanh::update(float) {}

Tensor& Tanh::get_weights() {
    throw std::runtime_error("Tanh has no weights");
}

Tensor& Tanh::get_grad_weights() {
    throw std::runtime_error("Tanh has no gradient weights");
}

void Tanh::set_weights(const Tensor& weights) {
    throw std::runtime_error("Tanh has no weights to set");
}



================================================
FILE: cpp/src/autograd.cpp
================================================
#include "autograd.h"

void Autograd::zero_grad(std::vector<Tensor>& tensors) {
    for (Tensor& tensor : tensors) {
        tensor.zero_grad();
    }
}



================================================
FILE: cpp/src/conv2d.cpp
================================================
#include "conv2d.h"
#include <Eigen/Dense>
#include <stdexcept>
#include <vector>

Conv2d::Conv2d(int in_channels, int out_channels, int kernel_size)
    : kernel_size_(kernel_size), learning_rate_(0.0f) {
    if (kernel_size % 2 == 0) {
        throw std::invalid_argument("Kernel size must be odd");
    }
    std::vector<int> weight_shape = {out_channels, in_channels, kernel_size, kernel_size};
    std::vector<int> bias_shape = {out_channels};
    weights_ = Tensor(weight_shape);
    bias_ = Tensor(bias_shape);
    grad_weights_ = Tensor(weight_shape);
    grad_bias_ = Tensor(bias_shape);

    // Initialisation des poids (exemple : initialisation aléatoire simple)
    for (int i = 0; i < weights_.size(); ++i) {
        weights_[i] = static_cast<float>(rand()) / RAND_MAX - 0.5f;
    }
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] = 0.0f;
    }
}

void Conv2d::forward(Tensor& input, Tensor& output) {
    if (input.ndim() != 4) {
        throw std::invalid_argument("Input must be 4D (batch, channels, height, width)");
    }
    int batch_size = input.shape()[0];
    int in_channels = input.shape()[1];
    int height = input.shape()[2];
    int width = input.shape()[3];
    int out_channels = weights_.shape()[0];
    int out_height = height - kernel_size_ + 1;
    int out_width = width - kernel_size_ + 1;

    if (output.shape() != std::vector<int>{batch_size, out_channels, out_height, out_width}) {
        throw std::invalid_argument("Output shape mismatch");
    }

    // Conversion des tenseurs en matrices Eigen pour le calcul
    Eigen::Map<Eigen::MatrixXf> weights_mat(weights_.data().data(), out_channels, in_channels * kernel_size_ * kernel_size_);
    Eigen::Map<Eigen::MatrixXf> output_mat(output.data().data(), batch_size * out_height * out_width, out_channels);

    // Extraction des patches de l'entrée (im2col)
    std::vector<float> input_patches(batch_size * out_height * out_width * in_channels * kernel_size_ * kernel_size_);
    for (int b = 0; b < batch_size; ++b) {
        for (int h = 0; h < out_height; ++h) {
            for (int w = 0; w < out_width; ++w) {
                for (int c = 0; c < in_channels; ++c) {
                    for (int kh = 0; kh < kernel_size_; ++kh) {
                        for (int kw = 0; kw < kernel_size_; ++kw) {
                            int idx = b * out_height * out_width * in_channels * kernel_size_ * kernel_size_ +
                                     (h * out_width + w) * in_channels * kernel_size_ * kernel_size_ +
                                     c * kernel_size_ * kernel_size_ + kh * kernel_size_ + kw;
                            int input_idx = b * in_channels * height * width +
                                           c * height * width +
                                           (h + kh) * width + (w + kw);
                            input_patches[idx] = input[input_idx];
                        }
                    }
                }
            }
        }
    }

    Eigen::Map<Eigen::MatrixXf> input_patches_mat(input_patches.data(),
        batch_size * out_height * out_width, in_channels * kernel_size_ * kernel_size_);
    output_mat = input_patches_mat * weights_mat.transpose();

    // Ajout du biais
    Eigen::Map<Eigen::VectorXf> bias_vec(bias_.data().data(), bias_.shape()[0]);
    for (int i = 0; i < batch_size * out_height * out_width; ++i) {
        output_mat.row(i) += bias_vec.transpose();
    }
}

void Conv2d::backward(Tensor& grad_output, Tensor& grad_input) {
    if (grad_output.ndim() != 4 || grad_input.ndim() != 4) {
        throw std::invalid_argument("Gradients must be 4D");
    }
    int batch_size = grad_input.shape()[0];
    int in_channels = grad_input.shape()[1];
    int height = grad_input.shape()[2];
    int width = grad_input.shape()[3];
    int out_channels = weights_.shape()[0];
    int out_height = height - kernel_size_ + 1;
    int out_width = width - kernel_size_ + 1;

    if (grad_output.shape() != std::vector<int>{batch_size, out_channels, out_height, out_width}) {
        throw std::invalid_argument("Gradient output shape mismatch");
    }

    // Conversion des tenseurs en matrices Eigen
    Eigen::Map<Eigen::MatrixXf> grad_output_mat(grad_output.data().data(), batch_size * out_height * out_width, out_channels);
    Eigen::Map<Eigen::MatrixXf> grad_input_mat(grad_input.data().data(), batch_size * height * width, in_channels);
    Eigen::Map<Eigen::MatrixXf> grad_weights_mat(grad_weights_.data().data(), out_channels, in_channels * kernel_size_ * kernel_size_);
    Eigen::Map<Eigen::VectorXf> grad_bias_vec(grad_bias_.data().data(), grad_bias_.shape()[0]);

    // Calcul du gradient des poids et du biais
    std::vector<float> input_patches(batch_size * out_height * out_width * in_channels * kernel_size_ * kernel_size_, 0.0f);
    Eigen::Map<Eigen::MatrixXf> input_patches_mat(input_patches.data(),
        batch_size * out_height * out_width, in_channels * kernel_size_ * kernel_size_);
    Eigen::Map<Eigen::MatrixXf> weights_mat(weights_.data().data(), out_channels, in_channels * kernel_size_ * kernel_size_);

    // Gradient du biais
    grad_bias_vec = grad_output_mat.colwise().sum();

    // Gradient des poids
    grad_weights_mat = grad_output_mat.transpose() * input_patches_mat;

    // Gradient de l'entrée
    grad_input_mat.setZero();
    Eigen::MatrixXf grad_input_patches = grad_output_mat * weights_mat;
    for (int b = 0; b < batch_size; ++b) {
        for (int h = 0; h < out_height; ++h) {
            for (int w = 0; w < out_width; ++w) {
                for (int c = 0; c < in_channels; ++c) {
                    for (int kh = 0; kh < kernel_size_; ++kh) {
                        for (int kw = 0; kw < kernel_size_; ++kw) {
                            int idx = b * out_height * out_width * in_channels * kernel_size_ * kernel_size_ +
                                     (h * out_width + w) * in_channels * kernel_size_ * kernel_size_ +
                                     c * kernel_size_ * kernel_size_ + kh * kernel_size_ + kw;
                            int grad_idx = b * in_channels * height * width +
                                          c * height * width +
                                          (h + kh) * width + (w + kw);
                            grad_input_mat(grad_idx, c) += grad_input_patches(idx, c);
                        }
                    }
                }
            }
        }
    }
}

void Conv2d::update(float lr) {
    learning_rate_ = lr;
    for (int i = 0; i < weights_.size(); ++i) {
        weights_[i] -= lr * grad_weights_[i];
    }
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] -= lr * grad_bias_[i];
    }
}

Tensor& Conv2d::get_weights() { return weights_; }
Tensor& Conv2d::get_grad_weights() { return grad_weights_; }

void Conv2d::set_weights(const Tensor& weights) {
    this->weights_ = weights;
}



================================================
FILE: cpp/src/data_loader.cpp
================================================
#include "data_loader.h"
#include <fstream>
#include <sstream>
#include <stdexcept>

DataLoader::DataLoader(const std::string& filename, int batch_size)
    : batch_size_(batch_size), current_index_(0) {
    std::ifstream file(filename);
    if (!file.is_open()) {
        throw std::runtime_error("Cannot open file: " + filename);
    }

    std::string line;
    if (std::getline(file, line)) {
        // La première ligne est ignorée
    }
    while (std::getline(file, line)) {
        std::vector<float> row;
        std::stringstream ss(line);
        std::string value;
        while (std::getline(ss, value, ',')) {
            row.push_back(std::stof(value));
        }
        data_.push_back(row);
    }
    file.close();
}

std::pair<Tensor, Tensor> DataLoader::next() {
    if (current_index_ >= static_cast<int>(data_.size())) {
        current_index_ = 0; // Reset pour boucler
    }

    int batch_end = std::min(current_index_ + batch_size_, static_cast<int>(data_.size()));
    std::vector<float> inputs;
    std::vector<float> targets;
    std::vector<int> input_shape = {batch_end - current_index_, static_cast<int>(data_[0].size()) - 1};
    std::vector<int> target_shape = {batch_end - current_index_, 1};

    for (int i = current_index_; i < batch_end; ++i) {
        for (size_t j = 0; j < data_[i].size() - 1; ++j) {
            inputs.push_back(data_[i][j]);
        }
        targets.push_back(data_[i].back());
    }

    current_index_ = batch_end;
    return {Tensor(input_shape, inputs), Tensor(target_shape, targets)};
}



================================================
FILE: cpp/src/linear.cpp
================================================
#include "linear.h"
#include <Eigen/Dense>
#include <stdexcept>

Linear::Linear(int in_features, int out_features) : learning_rate_(0.0f) {
    std::vector<int> weight_shape = {out_features, in_features};
    std::vector<int> bias_shape = {out_features};
    weights_ = Tensor(weight_shape);
    bias_ = Tensor(bias_shape);
    grad_weights_ = Tensor(weight_shape);
    grad_bias_ = Tensor(bias_shape);

    // Initialisation des poids (exemple : initialisation aléatoire simple)
    for (int i = 0; i < weights_.size(); ++i) {
        weights_[i] = static_cast<float>(rand()) / RAND_MAX - 0.5f;
    }
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] = 0.0f;
    }
}

void Linear::forward(Tensor& input, Tensor& output) {
    if (input.ndim() != 2) {
        throw std::invalid_argument("Input must be 2D (batch_size, in_features)");
    }
    int batch_size = input.shape()[0];
    int in_features = input.shape()[1];
    int out_features = weights_.shape()[0];

    if (output.shape() != std::vector<int>{batch_size, out_features}) {
        throw std::invalid_argument("Output shape mismatch");
    }

    Eigen::Map<Eigen::MatrixXf> input_mat(input.data().data(), batch_size, in_features);
    Eigen::Map<Eigen::MatrixXf> weights_mat(weights_.data().data(), out_features, in_features);
    Eigen::Map<Eigen::MatrixXf> output_mat(output.data().data(), batch_size, out_features);
    Eigen::Map<Eigen::VectorXf> bias_vec(bias_.data().data(), bias_.shape()[0]);

    output_mat = input_mat * weights_mat.transpose();
    output_mat.rowwise() += bias_vec.transpose();
}

void Linear::backward(Tensor& grad_output, Tensor& grad_input) {
    if (grad_output.ndim() != 2 || grad_input.ndim() != 2) {
        throw std::invalid_argument("Gradients must be 2D");
    }
    int batch_size = grad_input.shape()[0];
    int in_features = grad_input.shape()[1];
    int out_features = weights_.shape()[0];

    if (grad_output.shape() != std::vector<int>{batch_size, out_features}) {
        throw std::invalid_argument("Gradient output shape mismatch");
    }

    Eigen::Map<Eigen::MatrixXf> grad_output_mat(grad_output.data().data(), batch_size, out_features);
    Eigen::Map<Eigen::MatrixXf> grad_input_mat(grad_input.data().data(), batch_size, in_features);
    Eigen::Map<Eigen::MatrixXf> weights_mat(weights_.data().data(), out_features, in_features);
    Eigen::Map<Eigen::MatrixXf> grad_weights_mat(grad_weights_.data().data(), out_features, in_features);
    Eigen::Map<Eigen::VectorXf> grad_bias_vec(grad_bias_.data().data(), grad_bias_.shape()[0]);

    grad_input_mat = grad_output_mat * weights_mat;
    grad_weights_mat = grad_output_mat.transpose() * grad_input_mat;
    grad_bias_vec = grad_output_mat.colwise().sum();
}

void Linear::update(float lr) {
    learning_rate_ = lr;
    for (int i = 0; i < weights_.size(); ++i) {
        weights_[i] -= lr * grad_weights_[i];
    }
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] -= lr * grad_bias_[i];
    }
}

Tensor& Linear::get_weights() { return weights_; }
Tensor& Linear::get_grad_weights() { return grad_weights_; }

void Linear::set_weights(const Tensor& weights) {
    this->weights_ = weights;
}



================================================
FILE: cpp/src/loss.cpp
================================================
#include "loss.h"
#include <cmath>
#include <stdexcept>
#include <numeric>

// MSELoss

float MSELoss::forward(Tensor& y_pred, Tensor& y_true) {
    if (y_pred.shape() != y_true.shape()) {
        throw std::invalid_argument("Prediction and target shapes must match");
    }
    float loss = 0.0f;
    for (int i = 0; i < y_pred.size(); ++i) {
        float diff = y_pred[i] - y_true[i];
        loss += diff * diff;
    }
    return loss / y_pred.size();
}

Tensor MSELoss::backward(Tensor& y_pred, Tensor& y_true) {
    if (y_pred.shape() != y_true.shape()) {
        throw std::invalid_argument("Prediction and target shapes must match");
    }
    Tensor grad(y_pred.shape());
    for (int i = 0; i < y_pred.size(); ++i) {
        grad[i] = 2.0f * (y_pred[i] - y_true[i]) / y_pred.size();
    }
    return grad;
}

// CrossEntropyLoss

float CrossEntropyLoss::forward(Tensor& y_pred, Tensor& y_true) {
    if (y_pred.shape() != y_true.shape()) {
        throw std::invalid_argument("Prediction and target shapes must match");
    }
    float loss = 0.0f;
    float sum_exp = std::accumulate(y_pred.data().begin(), y_pred.data().end(), 0.0f,
        [](float sum, float x) { return sum + std::exp(x); });
    for (int i = 0; i < y_pred.size(); ++i) {
        float softmax = std::exp(y_pred[i]) / sum_exp;
        if (y_true[i] > 0.5f) {
            loss -= std::log(std::max(softmax, 1e-7f));
        }
    }
    return loss;
}

Tensor CrossEntropyLoss::backward(Tensor& y_pred, Tensor& y_true) {
    if (y_pred.shape() != y_true.shape()) {
        throw std::invalid_argument("Prediction and target shapes must match");
    }
    Tensor grad(y_pred.shape());
    float sum_exp = std::accumulate(y_pred.data().begin(), y_pred.data().end(), 0.0f,
        [](float sum, float x) { return sum + std::exp(x); });
    for (int i = 0; i < y_pred.size(); ++i) {
        float softmax = std::exp(y_pred[i]) / sum_exp;
        grad[i] = (y_true[i] > 0.5f) ? (softmax - 1.0f) : softmax;
    }
    return grad;
}



================================================
FILE: cpp/src/napca_sim.cpp
================================================
#include "napca_sim.h"
#include <Eigen/Dense>
#include <cmath>
#include <algorithm>
#include <unordered_set>

NAPCA_Sim::NAPCA_Sim(int in_features, int out_features, float alpha, float threshold)
    : learning_rate_(0.0f), alpha_(alpha), threshold_(threshold) {
    
    std::vector<int> weight_shape = {out_features, in_features};
    std::vector<int> bias_shape = {out_features};
    
    weights_ = Tensor(weight_shape);
    bias_ = Tensor(bias_shape);
    grad_weights_ = Tensor(weight_shape);
    grad_bias_ = Tensor(bias_shape);

    // Initialisation adaptative
    for (int i = 0; i < weights_.size(); ++i) {
        weights_[i] = (static_cast<float>(rand()) / RAND_MAX - 0.5f) * 0.1f;
        active_connections_[i] = true;
    }
    
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] = 0.0f;
    }
}

void NAPCA_Sim::forward(Tensor& input, Tensor& output) {
    // Implémentation du forward avec calcul simplifié
    std::vector<int> current_path;
    
    Eigen::Map<Eigen::MatrixXf> input_mat(input.data().data(), input.shape()[0], input.shape()[1]);
    Eigen::Map<Eigen::MatrixXf> weights_mat(weights_.data().data(), weights_.shape()[0], weights_.shape()[1]);
    Eigen::Map<Eigen::MatrixXf> output_mat(output.data().data(), output.shape()[0], output.shape()[1]);
    
    // Calcul simplifié avec alpha
    for (int i = 0; i < input.size(); ++i) {
        if (active_connections_[i % weights_.size()]) {
            float val = std::copysign(std::pow(std::abs(input[i]), alpha_), weights_[i % weights_.size()]);
            output[i % output.size()] += val;
            
            // Enregistrement du chemin
            if (val > threshold_) {
                current_path.push_back(i % weights_.size());
            }
        }
    }
    
    // Application du seuil
    for (int i = 0; i < output.size(); ++i) {
        output[i] = (output[i] > threshold_) ? 1.0f : 0.0f;
    }
    
    // Mémorisation du chemin
    memory_paths_.push_back(current_path);
}

void NAPCA_Sim::backward(Tensor& grad_output, Tensor& grad_input) {
    // Implémentation spécifique du backward
    for (int i = 0; i < grad_output.size(); ++i) {
        if (active_connections_[i % weights_.size()]) {
            grad_weights_[i % grad_weights_.size()] += 
                grad_output[i] * std::copysign(std::pow(std::abs(grad_input[i]), alpha_), weights_[i % weights_.size()]);
        }
    }
}

void NAPCA_Sim::update(float lr) {
    // Mise à jour des poids avec le taux d'apprentissage
    learning_rate_ = lr;
    for (int i = 0; i < weights_.size(); ++i) {
        if (active_connections_[i]) {
            weights_[i] -= learning_rate_ * grad_weights_[i];
        }
    }
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] -= learning_rate_ * grad_bias_[i];
    }
}

Tensor& NAPCA_Sim::get_weights() {
    return weights_;
}

Tensor& NAPCA_Sim::get_grad_weights() {
    return grad_weights_;
}

void NAPCA_Sim::set_weights(const Tensor& weights) {
    weights_ = weights;
}

float NAPCA_Sim::compute_path_similarity(const std::vector<int>& path1, const std::vector<int>& path2) {
    // Implémentation de la similarité Jaccard
    int intersection = 0;
    std::unordered_set<int> union_set(path1.begin(), path1.end());
    union_set.insert(path2.begin(), path2.end());
    
    for (int id : path1) {
        if (std::find(path2.begin(), path2.end(), id) != path2.end()) {
            intersection++;
        }
    }
    
    return union_set.empty() ? 0.0f : static_cast<float>(intersection) / union_set.size();
}

void NAPCA_Sim::update_weights_conditionally(const std::vector<std::pair<int,int>>& similar_pairs, float eta) {
    // Mise à jour conditionnelle des poids
    for (const auto& pair : similar_pairs) {
        const auto& path1 = memory_paths_[pair.first];
        const auto& path2 = memory_paths_[pair.second];
        
        for (int id : path1) {
            if (std::find(path2.begin(), path2.end(), id) == path2.end()) {
                weights_[id] -= eta * grad_weights_[id];
            }
        }
    }
}

void NAPCA_Sim::prune_connections(float threshold) {
    // Élagage des connexions peu utilisées
    for (auto& [id, active] : active_connections_) {
        if (std::abs(weights_[id]) < threshold) {
            active = false;
        }
    }
}



================================================
FILE: cpp/src/napcas.cpp
================================================
#include "napcas.h"
#include <Eigen/Dense>
#include <stdexcept>

NAPCAS::NAPCAS(int in_features, int out_features) : learning_rate_(0.0f) {
    std::vector<int> weight_shape = {out_features, in_features};
    std::vector<int> bias_shape = {out_features};
    weights_ = Tensor(weight_shape);
    bias_ = Tensor(bias_shape);
    grad_weights_ = Tensor(weight_shape);
    grad_bias_ = Tensor(bias_shape);

    // Initialisation des poids (exemple : initialisation aléatoire simple)
    for (int i = 0; i < weights_.size(); ++i) {
        weights_[i] = static_cast<float>(rand()) / RAND_MAX - 0.5f;
    }
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] = 0.0f;
    }
}

void NAPCAS::forward(Tensor& input, Tensor& output) {
    if (input.ndim() != 2) {
        throw std::invalid_argument("Input must be 2D (batch_size, in_features)");
    }
    int batch_size = input.shape()[0];
    int in_features = input.shape()[1];
    int out_features = weights_.shape()[0];

    if (output.shape() != std::vector<int>{batch_size, out_features}) {
        throw std::invalid_argument("Output shape mismatch");
    }

    Eigen::Map<Eigen::MatrixXf> input_mat(input.data().data(), batch_size, in_features);
    Eigen::Map<Eigen::MatrixXf> weights_mat(weights_.data().data(), out_features, in_features);
    Eigen::Map<Eigen::MatrixXf> output_mat(output.data().data(), batch_size, out_features);
    Eigen::Map<Eigen::VectorXf> bias_vec(bias_.data().data(), bias_.shape()[0]);

    output_mat = input_mat * weights_mat.transpose();
    output_mat.rowwise() += bias_vec.transpose();
}

void NAPCAS::backward(Tensor& grad_output, Tensor& grad_input) {
    if (grad_output.ndim() != 2 || grad_input.ndim() != 2) {
        throw std::invalid_argument("Gradients must be 2D");
    }
    int batch_size = grad_input.shape()[0];
    int in_features = grad_input.shape()[1];
    int out_features = weights_.shape()[0];

    if (grad_output.shape() != std::vector<int>{batch_size, out_features}) {
        throw std::invalid_argument("Gradient output shape mismatch");
    }

    Eigen::Map<Eigen::MatrixXf> grad_output_mat(grad_output.data().data(), batch_size, out_features);
    Eigen::Map<Eigen::MatrixXf> grad_input_mat(grad_input.data().data(), batch_size, in_features);
    Eigen::Map<Eigen::MatrixXf> weights_mat(weights_.data().data(), out_features, in_features);
    Eigen::Map<Eigen::MatrixXf> grad_weights_mat(grad_weights_.data().data(), out_features, in_features);
    Eigen::Map<Eigen::VectorXf> grad_bias_vec(grad_bias_.data().data(), grad_bias_.shape()[0]);

    grad_input_mat = grad_output_mat * weights_mat;
    grad_weights_mat = grad_output_mat.transpose() * grad_input_mat;
    grad_bias_vec = grad_output_mat.colwise().sum();
}

void NAPCAS::update(float lr) {
    learning_rate_ = lr;
    for (int i = 0; i < weights_.size(); ++i) {
        weights_[i] -= lr * grad_weights_[i];
    }
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] -= lr * grad_bias_[i];
    }
}

Tensor& NAPCAS::get_weights() { return weights_; }
Tensor& NAPCAS::get_grad_weights() { return grad_weights_; }

void NAPCAS::set_weights(const Tensor& weights) {
    this->weights_ = weights;
}



================================================
FILE: cpp/src/nncell.cpp
================================================
#include "nncell.h"
#include <Eigen/Dense>
#include <stdexcept>

NNCell::NNCell(int in_features, int out_features) : learning_rate_(0.0f) {
    std::vector<int> weight_shape = {out_features, in_features};
    std::vector<int> bias_shape = {out_features};
    weights_ = Tensor(weight_shape);
    bias_ = Tensor(bias_shape);
    grad_weights_ = Tensor(weight_shape);
    grad_bias_ = Tensor(bias_shape);

    // Initialisation des poids (exemple : initialisation aléatoire simple)
    for (int i = 0; i < weights_.size(); ++i) {
        weights_[i] = static_cast<float>(rand()) / RAND_MAX - 0.5f;
    }
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] = 0.0f;
    }
}

void NNCell::forward(Tensor& input, Tensor& output) {
    if (input.ndim() != 2) {
        throw std::invalid_argument("Input must be 2D (batch_size, in_features)");
    }
    int batch_size = input.shape()[0];
    int in_features = input.shape()[1];
    int out_features = weights_.shape()[0];

    if (output.shape() != std::vector<int>{batch_size, out_features}) {
        throw std::invalid_argument("Output shape mismatch");
    }

    Eigen::Map<Eigen::MatrixXf> input_mat(input.data().data(), batch_size, in_features);
    Eigen::Map<Eigen::MatrixXf> weights_mat(weights_.data().data(), out_features, in_features);
    Eigen::Map<Eigen::MatrixXf> output_mat(output.data().data(), batch_size, out_features);
    Eigen::Map<Eigen::VectorXf> bias_vec(bias_.data().data(), bias_.shape()[0]);

    output_mat = input_mat * weights_mat.transpose();
    output_mat.rowwise() += bias_vec.transpose();
}

void NNCell::backward(Tensor& grad_output, Tensor& grad_input) {
    if (grad_output.ndim() != 2 || grad_input.ndim() != 2) {
        throw std::invalid_argument("Gradients must be 2D");
    }
    int batch_size = grad_input.shape()[0];
    int in_features = grad_input.shape()[1];
    int out_features = weights_.shape()[0];

    if (grad_output.shape() != std::vector<int>{batch_size, out_features}) {
        throw std::invalid_argument("Gradient output shape mismatch");
    }

    Eigen::Map<Eigen::MatrixXf> grad_output_mat(grad_output.data().data(), batch_size, out_features);
    Eigen::Map<Eigen::MatrixXf> grad_input_mat(grad_input.data().data(), batch_size, in_features);
    Eigen::Map<Eigen::MatrixXf> weights_mat(weights_.data().data(), out_features, in_features);
    Eigen::Map<Eigen::MatrixXf> grad_weights_mat(grad_weights_.data().data(), out_features, in_features);
    Eigen::Map<Eigen::VectorXf> grad_bias_vec(grad_bias_.data().data(), grad_bias_.shape()[0]);

    grad_input_mat = grad_output_mat * weights_mat;
    grad_weights_mat = grad_output_mat.transpose() * grad_input_mat;
    grad_bias_vec = grad_output_mat.colwise().sum();
}

void NNCell::update(float lr) {
    learning_rate_ = lr;
    for (int i = 0; i < weights_.size(); ++i) {
        weights_[i] -= lr * grad_weights_[i];
    }
    for (int i = 0; i < bias_.size(); ++i) {
        bias_[i] -= lr * grad_bias_[i];
    }
}

Tensor& NNCell::get_weights() { return weights_; }
Tensor& NNCell::get_grad_weights() { return grad_weights_; }

void NNCell::set_weights(const Tensor& weights) {
    this->weights_ = weights;
}



================================================
FILE: cpp/src/optimizer.cpp
================================================
#include "optimizer.h"
#include <cmath>
#include <vector>
#include <memory>

// Constructeur par défaut pour SGD avec une learning rate par défaut
SGD::SGD(float lr) : learning_rate_(lr) {}

// Constructeur pour SGD avec une liste de modules et une learning rate par défaut
SGD::SGD(const std::vector<std::shared_ptr<Module>>& modules, float lr)
    : learning_rate_(lr), modules_(modules) {}

// Méthode pour effectuer un pas d'optimisation
void SGD::step() {
    for (auto& module : modules_) {
        auto weights = module->get_weights();
        auto grad_weights = module->get_grad_weights();
        for (size_t i = 0; i < weights.size(); ++i) {
            weights[i] -= learning_rate_ * grad_weights[i];
        }
        module->set_weights(weights);
    }
}

// Constructeur par défaut pour Adam avec des valeurs par défaut pour les paramètres
Adam::Adam(float lr, float beta1, float beta2, float epsilon)
    : learning_rate_(lr), beta1_(beta1), beta2_(beta2), epsilon_(epsilon), t_(0) {}

// Constructeur pour Adam avec une liste de modules et des valeurs par défaut pour les paramètres
Adam::Adam(const std::vector<std::shared_ptr<Module>>& modules, float lr, float beta1, float beta2, float epsilon)
    : learning_rate_(lr), beta1_(beta1), beta2_(beta2), epsilon_(epsilon), t_(0), modules_(modules) {
    for (auto& module : modules) {
        auto weights = module->get_weights();
        m_.push_back(Tensor(weights.shape()));
        v_.push_back(Tensor(weights.shape()));
    }
}

// Méthode pour effectuer un pas d'optimisation
void Adam::step() {
    t_++;
    for (size_t i = 0; i < modules_.size(); ++i) {
        auto module = modules_[i];
        auto weights = module->get_weights();
        auto grad_weights = module->get_grad_weights();
        for (size_t j = 0; j < weights.size(); ++j) {
            // Calcul des moments premiers et seconds
            m_[i][j] = beta1_ * m_[i][j] + (1 - beta1_) * grad_weights[j];
            v_[i][j] = beta2_ * v_[i][j] + (1 - beta2_) * grad_weights[j] * grad_weights[j];

            // Correction des biais
            float m_hat = m_[i][j] / (1 - std::pow(beta1_, t_));
            float v_hat = v_[i][j] / (1 - std::pow(beta2_, t_));

            // Mise à jour des poids
            weights[j] -= learning_rate_ * m_hat / (std::sqrt(v_hat) + epsilon_);
        }
        module->set_weights(weights);
    }
}



================================================
FILE: cpp/src/python_bindings.cpp
================================================
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <pybind11/functional.h>
#include <memory>

#include "tensor.h"
#include "module.h"
#include "linear.h"
#include "conv2d.h"
#include "activation.h"
#include "loss.h"
#include "optimizer.h"
#include "autograd.h"
#include "napcas.h"
#include "napca_sim.h"

namespace py = pybind11;
using namespace pybind11::literals;

PYBIND11_MODULE(napcas, m) {
    // Tensor
    py::class_<Tensor>(m, "Tensor")
        .def(py::init<const std::vector<int>&, const std::vector<float>&>(),
             "shape"_a, "data"_a = std::vector<float>())
        .def("shape", &Tensor::shape)
        .def("data", py::overload_cast<>(&Tensor::data), py::return_value_policy::reference_internal)
        .def("size", &Tensor::size)
        .def("fill", &Tensor::fill)
        .def("zero_grad", &Tensor::zero_grad);

    // Module de base
    py::class_<Module, std::shared_ptr<Module>>(m, "Module")
        .def("forward", &Module::forward)
        .def("backward", &Module::backward)
        .def("update", &Module::update)
        .def("get_weights", &Module::get_weights, py::return_value_policy::reference_internal)
        .def("get_grad_weights", &Module::get_grad_weights, py::return_value_policy::reference_internal)
        .def("set_weights", &Module::set_weights);

    // Modules dérivés
    py::class_<Linear, Module, std::shared_ptr<Linear>>(m, "Linear")
        .def(py::init<int, int>(), "in_features"_a, "out_features"_a);

    py::class_<Conv2d, Module, std::shared_ptr<Conv2d>>(m, "Conv2d")
        .def(py::init<int, int, int>(), "in_channels"_a, "out_channels"_a, "kernel_size"_a);

    py::class_<ReLU, Module, std::shared_ptr<ReLU>>(m, "ReLU")
        .def(py::init<>());

    py::class_<Sigmoid, Module, std::shared_ptr<Sigmoid>>(m, "Sigmoid")
        .def(py::init<>());

    py::class_<Tanh, Module, std::shared_ptr<Tanh>>(m, "Tanh")
        .def(py::init<>());

    // NAPCAS
    py::class_<NAPCAS, Module, std::shared_ptr<NAPCAS>>(m, "NAPCAS")
        .def(py::init<int, int>(), "in_features"_a, "out_features"_a)
        .def("forward", &NAPCAS::forward)
        .def("backward", &NAPCAS::backward)
        .def("update", &NAPCAS::update)
        .def("get_weights", &NAPCAS::get_weights, py::return_value_policy::reference_internal)
        .def("get_grad_weights", &NAPCAS::get_grad_weights, py::return_value_policy::reference_internal)
        .def("set_weights", &NAPCAS::set_weights);

    // NAPCA_Sim
    py::class_<NAPCA_Sim, Module, std::shared_ptr<NAPCA_Sim>>(m, "NAPCA_Sim")
        .def(py::init<int, int, float, float>(),
             "in_features"_a, "out_features"_a, "alpha"_a = 0.6f, "threshold"_a = 0.5f)
        .def("forward", &NAPCA_Sim::forward)
        .def("backward", &NAPCA_Sim::backward)
        .def("update", &NAPCA_Sim::update)
        .def("get_weights", &NAPCA_Sim::get_weights, py::return_value_policy::reference_internal)
        .def("get_grad_weights", &NAPCA_Sim::get_grad_weights, py::return_value_policy::reference_internal)
        .def("set_weights", &NAPCA_Sim::set_weights)
        .def("compute_path_similarity", &NAPCA_Sim::compute_path_similarity)
        .def("update_weights_conditionally", &NAPCA_Sim::update_weights_conditionally)
        .def("prune_connections", &NAPCA_Sim::prune_connections);

    // Fonctions de perte
    py::class_<MSELoss>(m, "MSELoss")
        .def(py::init<>())
        .def("forward", &MSELoss::forward)
        .def("backward", &MSELoss::backward);

    py::class_<CrossEntropyLoss>(m, "CrossEntropyLoss")
        .def(py::init<>())
        .def("forward", &CrossEntropyLoss::forward)
        .def("backward", &CrossEntropyLoss::backward);

    // Optimiseurs
    py::class_<SGD>(m, "SGD")
        .def(py::init<std::vector<std::shared_ptr<Module>>, float>(), "modules"_a, "lr"_a = 0.01f)
        .def("step", &SGD::step);

    py::class_<Adam>(m, "Adam")
        .def(py::init<std::vector<std::shared_ptr<Module>>, float, float, float, float>(),
             "modules"_a, "lr"_a = 0.001f, "beta1"_a = 0.9f, "beta2"_a = 0.999f, "epsilon"_a = 1e-8f)
        .def("step", &Adam::step);

    // Autograd
    m.def("zero_grad", &Autograd::zero_grad, "Efface les gradients", "tensors"_a);
}




================================================
FILE: cpp/src/tensor.cpp
================================================
#include "tensor.h"
#include <stdexcept>

Tensor::Tensor(const std::vector<int>& shape, const std::vector<float>& data) : shape_(shape) {
    int expected_size = 1;
    for (int dim : shape) {
        if (dim <= 0) {
            throw std::invalid_argument("Shape dimensions must be positive");
        }
        expected_size *= dim;
    }
    if (!data.empty() && data.size() != expected_size) {
        throw std::invalid_argument("Data size does not match shape");
    }
    data_.resize(expected_size);
    if (!data.empty()) {
        data_ = data;
    } else {
        std::fill(data_.begin(), data_.end(), 0.0f);
    }
}

void Tensor::fill(float value) {
    std::fill(data_.begin(), data_.end(), value);
}

void Tensor::zero_grad() {
    fill(0.0f);
}





================================================
FILE: python/__init__.py
================================================
from .napcas import Linear, Conv2d, ReLU, Sigmoid, Tanh, MSELoss, CrossEntropyLoss, SGD, Adam, DataLoader, Autograd, NAPCAS



================================================
FILE: python/pyproject.toml
================================================
[build-system]
requires = ["setuptools >= 64", "pybind11 >= 2.5.0"]
build-backend = "setuptools.build_meta"



================================================
FILE: python/setup.py
================================================
from setuptools import setup, Extension
from setuptools.command.build_ext import build_ext
import pybind11

__version__ = "0.1.0"

ext_modules = [
    Extension(
        '_napcas',
        ['../cpp/src/activation.cpp', 
        '../cpp/src/autograd.cpp', 
        '../cpp/src/conv2d.cpp', 
        '../cpp/src/data_loader.cpp', 
        '../cpp/src/linear.cpp', 
        '../cpp/src/loss.cpp', 
        '../cpp/src/nncell.cpp', 
        '../cpp/src/napcas.cpp', 
        '../cpp/src/napca_sim.cpp', 
        '../cpp/src/tensor.cpp', 
        '../cpp/src/python_bindings.cpp', 
        '../cpp/src/optimizer.cpp'],
        include_dirs=[
            '../cpp/include',
            pybind11.get_include(),
            pybind11.get_include(user=True),
            '/usr/include/eigen3',  # Chemin d'inclusion pour Eigen
        ],
        language='c++',
        extra_compile_args=['-std=c++17', '-fvisibility=hidden'],
        extra_link_args=['-std=c++17'],
    ),
]

setup(
    name='napcas',
    version=__version__,
    author='Komla M. DOLEAGBENOU',
    author_email='mkomla.doleagbenou@gmail.com',
    description='A sample Python project',
    long_description='',
    ext_modules=ext_modules,
    install_requires=['pybind11>=2.5.0'],
    setup_requires=['pybind11>=2.5.0'],
    cmdclass={'build_ext': build_ext},
    zip_safe=False,
)



================================================
FILE: python/test_dataloader_autograd.py
================================================
import pytest
import napcas
import os
import math

# Créer un fichier CSV temporaire pour tester DataLoader
@pytest.fixture
def temp_csv(tmp_path):
    data = "1.0,2.0,3.0\n4.0,5.0,6.0\n"
    file_path = tmp_path / "test.csv"
    with open(file_path, "w") as f:
        f.write(data)
    return str(file_path)

def test_dataloader(temp_csv):
    dataloader = napcas.DataLoader(temp_csv, 2)
    inputs, targets = dataloader.next()
    
    assert inputs.shape() == [2, 3], f"Expected input shape [2, 3], got {inputs.shape()}"
    assert targets.shape() == [2, 1], f"Expected target shape [2, 1], got {targets.shape()}"
    
    expected_inputs = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
    for i in range(inputs.size()):
        assert math.isclose(inputs[i], expected_inputs[i], rel_tol=1e-5), f"Input data mismatch at index {i}"

def test_autograd():
    # Créer un modèle simple
    linear = napcas.Linear(3, 2)
    input_tensor = napcas.Tensor([1, 3], [0.5, 0.5, 0.5])
    output_tensor = napcas.Tensor([1, 2])
    grad_output = napcas.Tensor([1, 2], [1.0, 1.0])
    
    # Forward
    linear.forward(input_tensor, output_tensor)
    
    # Backward avec Autograd
    napcas.Autograd.zero_grad()
    grad_input = napcas.Tensor(input_tensor.shape())
    linear.backward(grad_output, grad_input)
    
    # Vérifier que les gradients sont calculés
    grad_weights = linear.get_grad_weights()
    for i in range(grad_weights.size()):
        assert grad_weights[i] != 0.0, f"Gradient not computed at index {i}"
    
    # Vérifier zero_grad
    napcas.Autograd.zero_grad()
    grad_weights = linear.get_grad_weights()
    for i in range(grad_weights.size()):
        assert math.isclose(grad_weights[i], 0.0, rel_tol=1e-5), f"Gradient not zeroed at index {i}"



================================================
FILE: python/test_napcas.py
================================================
from napcas import Linear, ReLU, MSELoss, SGD, DataLoader
import napcas

# Test de la classe Linear
linear = Linear(784, 10)
input_tensor = napcas.Tensor([1, 784], [0.0] * 784)  # 2D tensor with batch size 1
output_tensor = napcas.Tensor([1, 10])  # 2D tensor for output
linear.forward(input_tensor, output_tensor)
print("Output of Linear:", [output_tensor[i] for i in range(output_tensor.size())])

# Test de ReLU
relu = ReLU()
output_relu = napcas.Tensor(input_tensor.shape())
relu.forward(input_tensor, output_relu)
print("Output of ReLU:", [output_relu[i] for i in range(output_relu.size())])

# Test de MSELoss
mse_loss = MSELoss()
y_pred = napcas.Tensor([1, 10], [0.5] * 10)  # 2D tensor with batch size 1
y_true = napcas.Tensor([1, 10], [1.0] * 10)  # 2D tensor with batch size 1
loss = mse_loss.forward(y_pred, y_true)
grad = mse_loss.backward(y_pred, y_true)
print("MSE Loss:", loss)
print("Gradient of MSE Loss:", [grad[i] for i in range(grad.size())])

# Test de DataLoader
data_loader = DataLoader("dataset.csv", 64)
inputs, targets = data_loader.next()
print("Inputs shape:", inputs.shape())
print("Targets shape:", targets.shape())



================================================
FILE: python/test_napcas_components.py
================================================
import pytest
import napcas

# Fonction pour vérifier les gradients par différences finies
def check_gradients(module, input_tensor, output_tensor, grad_output, epsilon=1e-5):
    module.forward(input_tensor, output_tensor)
    module.backward(grad_output, input_tensor)
    grad_weights = module.get_grad_weights().data()
    
    for i in range(min(10, grad_weights.size())):  # Limiter pour accélérer
        original_weight = module.get_weights()[i]
        module.get_weights()[i] = original_weight + epsilon
        module.forward(input_tensor, output_tensor)
        loss_plus = output_tensor.data().sum()
        module.get_weights()[i] = original_weight - epsilon
        module.forward(input_tensor, output_tensor)
        loss_minus = output_tensor.data().sum()
        module.get_weights()[i] = original_weight
        numerical_grad = (loss_plus - loss_minus) / (2 * epsilon)
        assert pytest.approx(grad_weights[i], rel=1e-3) == numerical_grad

def test_linear():
    linear = napcas.Linear(10, 5)
    input_tensor = napcas.Tensor([1, 10], [0.5] * 10)
    output_tensor = napcas.Tensor([1, 5])
    grad_output = napcas.Tensor([1, 5], [1.0] * 5)
    
    linear.forward(input_tensor, output_tensor)
    assert output_tensor.shape() == [1, 5]
    check_gradients(linear, input_tensor, output_tensor, grad_output)

def test_conv2d():
    conv = napcas.Conv2d(1, 16, 3)
    input_tensor = napcas.Tensor([1, 1, 28, 28], [0.5] * 784)
    output_tensor = napcas.Tensor([1, 16, 26, 26])
    grad_output = napcas.Tensor([1, 16, 26, 26], [1.0] * (16 * 26 * 26))
    
    conv.forward(input_tensor, output_tensor)
    assert output_tensor.shape() == [1, 16, 26, 26]
    check_gradients(conv, input_tensor, output_tensor, grad_output)

def test_conv2d_performance():
    conv = napcas.Conv2d(1, 16, 3)
    input_tensor = napcas.Tensor([1, 1, 28, 28], [0.5] * 784)
    output_tensor = napcas.Tensor([1, 16, 26, 26])
    grad_output = napcas.Tensor([1, 16, 26, 26], [1.0] * (16 * 26 * 26))
    grad_input = napcas.Tensor([1, 1, 28, 28])
    
    # Mesurer le temps de la rétropropagation
    start_time = time.time()
    for _ in range(100):  # Répéter pour une mesure fiable
        conv.forward(input_tensor, output_tensor)
        conv.backward(grad_output, grad_input)
    end_time = time.time()
    
    print(f"Conv2d backward time (100 iterations): {(end_time - start_time):.4f} seconds")
    assert end_time - start_time < 1.0, "Backward propagation too slow"

def test_relu():
    relu = napcas.ReLU()
    input_tensor = napcas.Tensor([1, 5], [-1.0, -0.5, 0.0, 0.5, 1.0])
    output_tensor = napcas.Tensor([1, 5])
    
    relu.forward(input_tensor, output_tensor)
    expected = [0.0, 0.0, 0.0, 0.5, 1.0]
    for i in range(5):
        assert pytest.approx(output_tensor[i], rel=1e-5) == expected[i]

def test_mse_loss():
    mse = napcas.MSELoss()
    y_pred = napcas.Tensor([1, 5], [0.5, 0.4, 0.3, 0.2, 0.1])
    y_true = napcas.Tensor([1, 5], [1.0, 0.8, 0.6, 0.4, 0.2])
    
    loss = mse.forward(y_pred, y_true)
    expected_loss = sum((p - t) ** 2 for p, t in zip(y_pred.data(), y_true.data())) / 5
    assert pytest.approx(loss, rel=1e-5) == expected_loss

def test_cross_entropy_loss():
    ce = napcas.CrossEntropyLoss()
    y_pred = napcas.Tensor([1, 3], [0.1, 0.2, 0.7])
    y_true = napcas.Tensor([1, 3], [0.0, 0.0, 1.0])
    
    loss = ce.forward(y_pred, y_true)
    softmax = [math.exp(y_pred[i]) / sum(math.exp(y_pred[j]) for j in range(3)) for i in range(3)]
    expected_loss = -math.log(softmax[2])
    assert pytest.approx(loss, rel=1e-5) == expected_loss

def test_sgd():
    linear = napcas.Linear(10, 5)
    sgd = napcas.SGD([linear], lr=0.01)
    weights_before = linear.get_weights().data().copy()
    
    input_tensor = napcas.Tensor([1, 10], [0.5] * 10)
    output_tensor = napcas.Tensor([1, 5])
    grad_output = napcas.Tensor([1, 5], [1.0] * 5)
    
    linear.forward(input_tensor, output_tensor)
    linear.backward(grad_output, input_tensor)
    sgd.step()
    
    weights_after = linear.get_weights().data()
    for i in range(len(weights_before)):
        assert weights_before[i] != weights_after[i], "Weights not updated"

def test_adam():
    linear = napcas.Linear(10, 5)
    adam = napcas.Adam([linear], lr=0.001)
    weights_before = linear.get_weights().data().copy()
    
    input_tensor = napcas.Tensor([1, 10], [0.5] * 10)
    output_tensor = napcas.Tensor([1, 5])
    grad_output = napcas.Tensor([1, 5], [1.0] * 5)
    
    linear.forward(input_tensor, output_tensor)
    linear.backward(grad_output, input_tensor)
    adam.step()
    
    weights_after = linear.get_weights().data()
    for i in range(len(weights_before)):
        assert weights_before[i] != weights_after[i], "Weights not updated"



================================================
FILE: tests/CMakeLists.txt
================================================
# CMakeLists.txt pour le répertoire tests

# Trouver Google Test
find_package(GTest REQUIRED)

# Ajouter les tests unitaires
add_executable(test_napcas test_all_modules.cpp test_napcas.cpp)
target_link_libraries(test_napcas GTest::GTest GTest::Main libnapcas)

# Configurer les tests avec CTest
enable_testing()
add_test(NAME test_napcas COMMAND test_napcas)



================================================
FILE: tests/test_all_modules.cpp
================================================
#include <gtest/gtest.h>
#include "linear.h"
#include "conv2d.h"
#include "activation.h"
#include "loss.h"
#include "optimizer.h"
#include "data_loader.h"
#include "autograd.h"
#include "tensor.h"

TEST(LinearTest, ForwardPass) {
    // Test code for Linear layer
}

TEST(Conv2dTest, ForwardPass) {
    // Test code for Conv2d layer
}

// Add more tests as needed



================================================
FILE: tests/test_all_modules.py
================================================
import napcas

# Test Linear
x = napcas.Tensor([1, 10], [0.5]*10)
linear = napcas.Linear(10, 5)
output = linear.forward(x)
print(output)

# Test Conv2d
conv = napcas.Conv2d(1, 16, 3)
input_tensor = napcas.Tensor([1, 1, 28, 28], [0.5] * 784)
output_tensor = napcas.Tensor([1, 16, 26, 26])
conv.forward(input_tensor, output_tensor)
print(output_tensor)

# Test Activation
relu = napcas.ReLU()
output = relu.forward(x)
print(output)

# Test Loss
loss = napcas.MSELoss()
loss_value = loss.forward(x, x)
print(loss_value)

# Test Optimizer
opt = napcas.SGD([linear])
opt.step()



================================================
FILE: tests/test_napcas.cpp
================================================
#include <gtest/gtest.h>
#include "napcas.h"

TEST(NAPCASTest, ForwardPass) {
    // Test code for NAPCAS layer
}

// Add more tests as needed



================================================
FILE: tests/test_napcas.py
================================================
import napcas

# Exemple d'utilisation
x = napcas.Tensor([1, 10], [0.5]*10)
y = napcas.Tensor([1, 5], [0.0]*5)

cell = napcas.NNCel(10, 5)
cell.forward(x)
cell.backward(y)
cell.update(0.01)
